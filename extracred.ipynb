{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "!pip install xgboost\n",
        "!pip install category_encoders\n",
        "!pip install imblearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, PowerTransformer\n",
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split\n",
        "from sklearn.metrics import f1_score, roc_auc_score, classification_report, precision_score, recall_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from category_encoders import TargetEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the training and test datasets\n",
        "train_df = pd.read_csv('data/train.csv')\n",
        "test_df = pd.read_csv('data/test.csv')\n",
        "\n",
        "# Feature Engineering\n",
        "def haversine_distance(lat1, lon1, lat2, lon2):\n",
        "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    km = 6371 * c\n",
        "    return km\n",
        "\n",
        "for df in [train_df, test_df]:\n",
        "    # Combine date and time columns\n",
        "    df['trans_datetime'] = pd.to_datetime(df['trans_date'] + ' ' + df['trans_time'])\n",
        "\n",
        "    # Calculate age from 'dob'\n",
        "    df['dob'] = pd.to_datetime(df['dob'], errors='coerce')\n",
        "    df['age'] = (df['trans_datetime'] - df['dob']).dt.days / 365.25\n",
        "\n",
        "    # Extract transaction time features\n",
        "    df['hour'] = df['trans_datetime'].dt.hour\n",
        "    df['day'] = df['trans_datetime'].dt.day\n",
        "    df['month'] = df['trans_datetime'].dt.month\n",
        "    df['weekday'] = df['trans_datetime'].dt.weekday\n",
        "\n",
        "    # Calculate distance using the Haversine formula\n",
        "    df['distance'] = haversine_distance(df['lat'], df['long'], df['merch_lat'], df['merch_long'])\n",
        "\n",
        "    # Sort transactions and calculate time since last transaction\n",
        "    df.sort_values(by=['cc_num', 'unix_time'], inplace=True)\n",
        "    df['time_since_last_txn'] = df.groupby('cc_num')['unix_time'].diff()\n",
        "\n",
        "    # Velocity (distance/time)\n",
        "    df['velocity'] = df['distance'] / (df['time_since_last_txn'] + 1)\n",
        "\n",
        "    # Aggregated Transaction Statistics\n",
        "    df['amt_mean_user'] = df.groupby('cc_num')['amt'].transform('mean')\n",
        "    df['amt_std_user'] = df.groupby('cc_num')['amt'].transform('std')\n",
        "    df['amt_ratio'] = df['amt'] / (df['amt_mean_user'] + 1)\n",
        "\n",
        "    # Merchant-Cardholder Interaction Features\n",
        "    df['merchant_cardholder_freq'] = df.groupby(['cc_num', 'merchant'])['id'].transform('count')\n",
        "    df['merchant_cardholder_avg_amt'] = df.groupby(['cc_num', 'merchant'])['amt'].transform('mean')\n",
        "\n",
        "# Encode categorical variables\n",
        "gender_map = {'F': 0, 'M': 1}\n",
        "for df in [train_df, test_df]:\n",
        "    df['gender'] = df['gender'].map(gender_map)\n",
        "\n",
        "# Target Encoding for categorical features\n",
        "categorical_features = ['category', 'job', 'state']\n",
        "encoder = TargetEncoder(cols=categorical_features)\n",
        "encoder.fit(train_df[categorical_features], train_df['is_fraud'])\n",
        "train_df[categorical_features] = encoder.transform(train_df[categorical_features])\n",
        "test_df[categorical_features] = encoder.transform(test_df[categorical_features])\n",
        "\n",
        "# Feature Transformation\n",
        "for df in [train_df, test_df]:\n",
        "    df['amt_log'] = np.log1p(df['amt'])\n",
        "    df['distance_log'] = np.log1p(df['distance'])\n",
        "\n",
        "# Additional Interaction Features\n",
        "for df in [train_df, test_df]:\n",
        "    df['hour_day_interaction'] = df['hour'] * df['day']\n",
        "    df['amt_age_interaction'] = df['amt'] * df['age']\n",
        "    df['amt_distance_interaction'] = df['amt'] * df['distance']\n",
        "\n",
        "# Select updated features\n",
        "features = [\n",
        "    'amt', 'amt_mean_user', 'amt_std_user', 'amt_ratio', 'gender', 'category', 'age', 'city_pop',\n",
        "    'job', 'hour', 'day', 'month', 'weekday', 'distance', 'state', 'amt_log', 'distance_log',\n",
        "    'hour_day_interaction', 'amt_age_interaction', 'amt_distance_interaction',\n",
        "    'merchant_cardholder_freq', 'merchant_cardholder_avg_amt', 'time_since_last_txn', 'velocity'\n",
        "]\n",
        "\n",
        "X = train_df[features]\n",
        "y = train_df['is_fraud']\n",
        "X_test = test_df[features]\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X = pd.DataFrame(imputer.fit_transform(X), columns=features)\n",
        "X_test = pd.DataFrame(imputer.transform(X_test), columns=features)\n",
        "\n",
        "# Normalize data using PowerTransformer\n",
        "scaler = PowerTransformer()\n",
        "X = pd.DataFrame(scaler.fit_transform(X), columns=features)\n",
        "X_test = pd.DataFrame(scaler.transform(X_test), columns=features)\n",
        "\n",
        "# Address class imbalance\n",
        "smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "undersampler = RandomUnderSampler(random_state=42, sampling_strategy='auto')\n",
        "X, y = smote.fit_resample(X, y)\n",
        "X, y = undersampler.fit_resample(X, y)\n",
        "\n",
        "# Split the training data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Hyperparameter tuning for XGBoost\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 500],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7, 10],\n",
        "    'subsample': [0.6, 0.8, 1],\n",
        "    'colsample_bytree': [0.6, 0.8, 1],\n",
        "    'gamma': [0, 0.1, 0.3, 0.5],\n",
        "    'reg_alpha': [0, 0.1, 0.5, 1, 2],\n",
        "    'reg_lambda': [1, 1.5, 2, 3, 5],\n",
        "    'min_child_weight': [1, 3, 5],\n",
        "    'scale_pos_weight': [1, 2, 5]\n",
        "}\n",
        "\n",
        "model = xgb.XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    model, param_distributions=param_dist, n_iter=200, scoring='f1',\n",
        "    n_jobs=-1, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), verbose=2\n",
        ")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the best model\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_val)\n",
        "y_pred_proba = best_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "print('Validation F1 Score:', f1_score(y_val, y_pred))\n",
        "print('Validation ROC AUC:', roc_auc_score(y_val, y_pred_proba))\n",
        "print('Classification Report:', classification_report(y_val, y_pred))\n",
        "print('Precision:', precision_score(y_val, y_pred))\n",
        "print('Recall:', recall_score(y_val, y_pred))\n",
        "\n",
        "# Predictions on test set\n",
        "test_preds = best_model.predict(X_test)\n",
        "submission = pd.DataFrame({'id': test_df['id'], 'is_fraud': test_preds})\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "l4w3e-0L49wF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}